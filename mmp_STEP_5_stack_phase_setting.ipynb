{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting of stack phase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this phase we are going to set the stacked-phase dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import getVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../dsbase/src/main')\n",
    "from AdaBoostClassificationDSBase import AdaBoostClassificationDSBaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the original stacked dataset and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/train_stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frac = df.sample(frac=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(815, 77)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataframe to list ...\n",
      "   1 column transformation ...\n",
      "   2 column transformation ...\n",
      "   3 column transformation ...\n",
      "   recomponing dataset ...\n",
      "   loading model ...\n",
      "initiating empty model AB2. AdaBoostClassification\n",
      "loading model: models/fold1/AdaBoostClassification_AB2.sav\n",
      "   getting rest of One-Hot ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator AdaBoostClassifier from version 0.20.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.20.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "fold_id=1\n",
    "\n",
    "# Convert every element to a one-elenet List\n",
    "print('   dataframe to list ...')\n",
    "df_w = df_frac.drop(['HasDetections','fold'], axis=1)\n",
    "df_aux = pd.DataFrame([list(map(lambda x: [x], row)) for row in df_w.values], columns=df_w.columns)\n",
    "\n",
    "# Load columns and process\n",
    "print('   1 column transformation ...')\n",
    "AvSigVersion = np.load('models/fold' + str(fold_id) + \"/AvSigVersion.sav.npy\")\n",
    "df_aux['AvSigVersion']=df_aux['AvSigVersion'].apply(lambda x: getVector(x[0],AvSigVersion))\n",
    "print('   2 column transformation ...')\n",
    "Census_OSVersion = np.load('models/fold' + str(fold_id) + \"/Census_OSVersion.sav.npy\")\n",
    "df_aux['Census_OSVersion']=df_aux['Census_OSVersion'].apply(lambda x: getVector(x[0],Census_OSVersion))\n",
    "print('   3 column transformation ...')\n",
    "OsBuildLab = np.load('models/fold' + str(fold_id) + \"/OsBuildLab.sav.npy\")\n",
    "df_aux['OsBuildLab']=df_aux['OsBuildLab'].apply(lambda x: getVector(x[0],OsBuildLab))\n",
    "\n",
    "# Set the adapted dataset\n",
    "print('   recomponing dataset ...')    \n",
    "df_end = pd.DataFrame([np.concatenate(row) for row in df_aux.values])\n",
    "\n",
    "# --------------------------------------\n",
    "# Load the i-th model and process\n",
    "print('   loading model ...')    \n",
    "model = AdaBoostClassificationDSBaseModel('AB2',None,None,None,None,None,None)\n",
    "model.load('models/fold' + str(1))\n",
    "\n",
    "# Preparing data to be predicted\n",
    "print('   getting rest of One-Hot ...')    \n",
    "df_data_to_predict = pd.get_dummies(df_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Fold X processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getColumnFoldX(df, fold_id):\n",
    "    # Convert every element to a one-elenet List\n",
    "    print('   dataframe to list ...')\n",
    "    df_w = df.drop(['HasDetections','fold'], axis=1)\n",
    "    df_aux = pd.DataFrame([list(map(lambda x: [x], row)) for row in df_w.values], columns=df_w.columns)\n",
    "    \n",
    "    # Load columns and process\n",
    "    print('   1 column transformation ...')\n",
    "    AvSigVersion = np.load('models/fold' + str(fold_id) + \"/AvSigVersion.sav.npy\")\n",
    "    df_aux['AvSigVersion']=df_aux['AvSigVersion'].apply(lambda x: getVector(x[0],AvSigVersion))\n",
    "    print('   2 column transformation ...')\n",
    "    Census_OSVersion = np.load('models/fold' + str(fold_id) + \"/Census_OSVersion.sav.npy\")\n",
    "    df_aux['Census_OSVersion']=df_aux['Census_OSVersion'].apply(lambda x: getVector(x[0],Census_OSVersion))\n",
    "    print('   3 column transformation ...')\n",
    "    OsBuildLab = np.load('models/fold' + str(fold_id) + \"/OsBuildLab.sav.npy\")\n",
    "    df_aux['OsBuildLab']=df_aux['OsBuildLab'].apply(lambda x: getVector(x[0],OsBuildLab))\n",
    "\n",
    "    # Set the adapted dataset\n",
    "    print('   recomponing dataset ...')    \n",
    "    df_end = pd.DataFrame([np.concatenate(row) for row in df_aux.values])\n",
    "    \n",
    "    # --------------------------------------\n",
    "    # Load the i-th model and process\n",
    "    print('   loading model ...')    \n",
    "    model = AdaBoostClassificationDSBaseModel('AB2',None,None,None,None,None,None)\n",
    "    model.load('models/fold' + str(1))\n",
    "    \n",
    "    # Preparing data to be predicted\n",
    "    print('   getting rest of One-Hot ...')    \n",
    "    df_data_to_predict = pd.get_dummies(df_end)\n",
    "  \n",
    "    print('   Calculating: normalization ...')    \n",
    "    pre_result = model.scalerX.transform(df_data_to_predict.values)\n",
    "    print('   Calculating: probabilities ...')    \n",
    "    result = model.model.predict_proba(pre_result)\n",
    "    \n",
    "    # Set the result as a one-column DataFrame\n",
    "    print('   Creating result dataset ...')        \n",
    "    columns_name = [str('f' + str(fold_id))]\n",
    "    df_result = pd.DataFrame(result)\n",
    "    df_result.columns = columns_name\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataframe to list ...\n",
      "   1 column transformation ...\n",
      "   2 column transformation ...\n",
      "   3 column transformation ...\n",
      "   recomponing dataset ...\n",
      "   loading model ...\n",
      "initiating empty model AB2. AdaBoostClassification\n",
      "loading model: models/fold1/AdaBoostClassification_AB2.sav\n",
      "   getting rest of One-Hot ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator AdaBoostClassifier from version 0.20.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.20.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Calculating: normalization ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (815,11451) (6861,) (815,11451) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-67357f926354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetColumnFoldX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_frac\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-2aa25d51cd9d>\u001b[0m in \u001b[0;36mgetColumnFoldX\u001b[0;34m(df, fold_id)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'   Calculating: normalization ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mpre_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalerX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data_to_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'   Calculating: probabilities ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (815,11451) (6861,) (815,11451) "
     ]
    }
   ],
   "source": [
    "f1 = getColumnFoldX(df_frac,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets obtain the final stacked dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 9 # Number of folds\n",
    "df_stack_set = df_frac\n",
    "for i in range(9):\n",
    "    print('processing fold ' + str(i+1) + \" ...\")\n",
    "    c = getColumnFoldX(df_frac, i+1)\n",
    "    df_stack_set = df_stack_set.join(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack_set.drop(['fold'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
